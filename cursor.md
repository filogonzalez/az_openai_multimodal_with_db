Version 1.0
DSPy-Free Multimodal LLM RAG Pipeline Migration Checklist
Project Setup and Branch Preparation
•	Work in main root folder: (e.g., /AZ_OPENAI_MULTIMODAL_WITH_DB) to isolate refactor changes.
•	Set up config files: Plan to consolidate config.py into a unified config (JSON-like dict) in global_config.py for both Databricks and local use. Use the existing global_config.py structure (nested dictionaries) as a model. Mark config.py for deprecation (add a # TODO: Remove DSPy config.py after merging into global_config).
•	Dual environment prep: Identify differences between Databricks and local Cursor dev. For example, use environment flags or try/except on dbutils to handle platform-specific code (e.g., widget setup and dbutils calls). Document these with inline TODO comments.
•	Considerations: legacy_code is the “old” code and MultiModal_DSPy need DSPy removed. We leverage both to build our new optimized code. 
Global Configuration Refactor (global_config.py)
•	Merge DSPy config values: Integrate values from config.py (CLIP model names, table names, endpoints) into global_config.py as a structured dict. For example, add keys for clip_model_endpoint, embedding_table, vector_index_name, etc., under a new section or existing ones. Ensure the format is JSON-style (nested dict) for clarity.
•	Replace YAML config generation: If global_config.py currently writes YAML files, decide if still needed. For simpler config management, consider directly storing config in a Python dict or a JSON file. Update any config loading code to use the unified config source (e.g., remove loading of separate DSPy config.py).
•	Remove legacy text config: Remove or update fields related to text chunking. For instance, eliminate retriever_config['schema']['chunk_text'] and similar text-focused keys, since PDF text will no longer be used. If needed, replace with image-related fields (e.g., a placeholder for image data if any).
•	Double-check endpoints: Verify that databricks_resources in config includes the correct LLM endpoint name (GPT-4o’s Azure deployment) and the new vector search endpoint name. Update these if needed (e.g., ensure llm_endpoint_name points to GPT-4o model and vector_search_endpoint_name matches the new CLIP index endpoint).
PDF Ingestion & Processing Module
(Replacing legacy streaming_document_pipeline.py with new image-based pipeline)
•	Remove text parsing logic: Delete or comment out functions that parse PDF text (e.g., parse_pdf_udf and text chunk UDFs in streaming_document_pipeline.py). The new pipeline will not use text content. Mark these removals with # TODO: Remove text parsing – using images only.
•	Incorporate image conversion: Reuse the logic from 02_PDF ETL.py to convert PDF pages to images and base64. Implement a function (e.g., process_pdf_to_images) that reads each PDF file, converts each page to a JPEG image, resizes if necessary, and base64-encodes. This function can run either on Databricks (driver node) or locally (with poppler installed).
o	Use pdf2image.convert_from_path as shown in the DSPy notebook 02_PDF ETL.py. Include error handling for missing files or unreadable files (as in the example) and log progress.
o	Ensure no text extraction: do not call PdfReader.extract_text() at all, or if you do, use it only for verification purposes. The goal is that base64_image is the primary content.
•	Schema updates: Define the Spark/Pandas DataFrame schema for the page data with appropriate fields: id, pdf_path, page_number, base64_image, and (optionally).Do not include text_chunk, adjust accordingly. Update any Delta table writes to use the new schema (e.g., writing to a table like clip_pdf_pages).
•	Mark streaming pipeline obsolete: If using Delta Live Tables before, note that this new processing is a batch job. Document in code (# NOTE: Replacing DLT streaming pipeline with batch image processing). You can remove DLT decorators and just use normal Spark writes for the new pipeline.
Embedding Generation with CLIP Model
•	Ensure CLIP model deployment: The CLIP MLflow model should be logged and deployed (e.g., via 01_CLIP_Deployment.py). If not already done, run or adapt that code to register the model and create a serving endpoint. Use mlflow.pyfunc.log_model as in the provided notebook to log the CLIPInferenceModel and create the endpoint.
o	Verify the model signature expects image_base64 and text inputs and returns embedding considering databricks limitations and best practices. No changes needed here except to confirm the model is accessible.
o	(Optional) Test the model locally: Use the sample code from the notebook to load the model and call predict on a known image to ensure it returns embeddings. This double-checks that the model’s predict method works outside DSPy.
•	Batch embedding pipeline: Refactor or reuse the process_embeddings function from 02_PDF ETL.py to generate embeddings for all page images. Key changes:
o	Remove any references to text in the input if not needed. In the original, it sends both image and text chunk; adjust the model to accept image alone.
o	Use mlflow.deployments.get_deploy_client("databricks") and client.predict to get embeddings for each batch. This remains the same.
o	Maintain the embedding results schema (with id, embeddings, etc.) and collect results into a list of dicts.
o	Replace forward with predict: If the CLIP model’s Python class currently defines a forward method (common in PyTorch/dspy modules), ensure that it has a predict method for MLflow to call. For example, in the DSPy MultiModal, logic was in forward, but our refactored version should expose it via a standard function or a predict(). In this case, since we call the deployed model via client.predict, this is handled by MLflow’s predict interface. Just verify the custom model (if any) uses predict properly. Mark any forward() implementations for conversion to predict().
•	Write embeddings to vector store: After obtaining embeddings, write them to the Delta table as done previously. Ensure the Delta table (e.g., <catalog>.<schema>.<embedding_table_name>) is the one your vector index will use. Use mergeSchema=true if schema changed (e.g., dropped text).
Vector Search Index Setup (Databricks Vector Search)
•	Endpoint creation: Use the Databricks Vector Search client to create/verify the vector search endpoint. In the legacy code, VECTOR_SEARCH_ENDPOINT was created if not exists. For the new pipeline, ensure vector_search_endpoint_name (from config) is created (likely it already exists if using the same name, but double-check). Use VectorSearchClient.create_endpoint_and_wait() without DSPy, similar to legacy code vector_search_manager.py.
•	Index creation: Create a new vector index for the image embeddings. Adapt the old create_vector_search_index logic: use create_delta_sync_index_and_wait with the new embedding table name, primary key, and embedding column. For example, use primary_key="id", embedding_vector_column="embeddings", and the fully qualified source_table_name for your embeddings Delta table.
o	Remove all references to chunked_text in this context. In the old code, embedding_source_column="chunked_text" was used file:vector_search_manager.py; now it should point to the embeddings vector field and the pipeline will auto-index vectors.
o	Ensure the index_name is updated (from embedding_table_name_index in config.py). This index name should be unique (e.g., clip_model_embedding_cpu_index as seen in DSPy code 04_DSPy Orchestration.py, or whatever was set in config). Confirm it matches across your config and code.
•	Hybrid search parameters: If you want to allow hybrid (text+vector) queries, you can still specify query_type="HYBRID" when doing similarity 02_PDF ETL.py. Since we have no text in the index, hybrid vs pure ANN may not matter, but you can include the original query text in the search call (the example passed query_text=input_query along with query_vector 02_PDF ETL.py). This can help if the vector index supports hybrid search.
•	Test index retrieval: After index creation, perform a simple similarity search query to validate. For example, use a sample question and get an embedding via the CLIP endpoint (as the code does) and call index.similarity_search(num_results=5, columns=["base64_image","pdf_path","page_number"], query_vector=<emb>, query_text=<text>). Ensure it returns results without error 04_DSPy Orchestration.py. Log or print the results to confirm images are retrieved (base64 strings).
Agent / Orchestration Module (agent.py Replacement)
(Refactor the question-answering agent to use images instead of text and remove DSPy)
•	Remove LangChain tool logic: The existing @agent.py likely uses LangChain or similar tools (e.g., VectorSearchRetrieverTool for text @agent.py). Eliminate these tool-based retrievals and the iterative agent loop, since we will directly use our vector search and LLM calls. Mark this with # TODO: Remove LangChain tool usage – using custom retrieval. Instead, plan for a simpler pipeline: query -> embed -> retrieve -> answer.
•	Integrate image retrieval logic: Implement a function or class method (e.g., retrieve_relevant_images(query: str)) that encapsulates the steps to get top K images for a query. You can base this on the vector_search_for_images method from the DSPy MultiModalAnalyzer @04_DSPy Orchestration.py:
1.	Use the MLflow deployment client to get the text embedding from the user query @04_DSPy Orchestration.py. (Call the CLIP model’s using the MLflow client is in Databricks because we configured it locally).
2.	Query the vector index for similar images. Request at least the base64_image and any metadata needed (PDF path, page number, etc.) @04_DSPy Orchestration.py.
3.	Return the list of results (each result containing the base64 image string and maybe source info).
•	Reuse ImageAnalyzer logic (without DSPy): Create a helper function (e.g., answer_with_image(image_b64: str, question: str) -> str) that sends the images and question to the GPT-4o model and returns the answer. This replaces DSPy’s ImageAnalyzer which took only one image and text and produced a response @04_DSPy Orchestration.py. Key points:
o	Format the input for the LLM. Since GPT-4o (or the Azure OpenAI model) is multimodal, find the correct way to pass all the image retrieved with the similarity_search. If Azure OpenAI allows image input, it might be via a special prompt or an API parameter. (For example, some OpenAI models allow a image field or a special <image> tag with base64.) Consult Azure OpenAI docs for image support and adjust accordingly. Use @ azopenaidriver.py to see how we pass our base64_string (encoded_image) to the azure open ai model.
o	Possibly, you might send a system message like: “Here is an image in base64: [data]. Answer the question based on this image: [question]”. This is speculative – adjust to the actual API.
o	Call the model: If using the Azure OpenAI ChatCompletion API, prepare the messages payload. If using ChatDatabricks (from databricks_langchain), instantiate it with the Claude endpoint (already configured in global_config) to get a predictor. For example, the old code does llm = ChatDatabricks(endpoint=..., extra_params=...)@agent.py. You can use a similar approach to call the model’s .predict.
o	No DSPy dependencies: Remove any dspy.Image or dspy.LM usage. For image, our input is a base64 string; no need to convert to dspy.Image. And for the LLM, use the direct Claude API or ChatDatabricks instead of dspy.LM.
•	Compose final answer: If multiple images are retrieved (e.g., top 3-5), you might want to combine answers or let the LLM see multiple images. There are two approaches:
1.	Iterate through each retrieved image, get an answer snippet for each, then combine. (The DSPy pipeline did this by analyzing each image and concatenating responses.) You can do similarly: accumulate answers in a list.
2.	Or, provide all images at once to the model. This depends on model capability and input limits. Simpler is the iterative approach: call answer_with_image for each and then merge results.
o	If merging, you can format the final response with each image’s answer and optionally a “Sources” section listing the PDFs/pages (like the DSPy code appended sources at the very end of the response). This helps users know which documents were referenced. Use the pdf_path and page_number metadata for this.
•	Implement predict method: If you encapsulate the above in a class (say MultimodalAgent), define a predict(self, query: str) -> str method that executes: images = retrieve_relevant_images(query), then for each image in images: answer_with_image(image, query), then aggregate. This predict will be the main entry point for Q&A. (This is effectively the replacement for MultiModalAnalyzer.forward, using standard Python method naming.) Add a # TODO: verify predict works with MLflow if exporting the agent as model if you plan to package it.
•	Clean up agent registration: If the old agent.py was setting up an MLflow model (e.g., mlflow.models.set_model(RAG_AGENT)agent.py), you can remove or refactor that. You might not need to register the agent as an MLflow model; but if you do, you’ll need to wrap your MultimodalAgent.predict in an MLflow pyfunc. This could be future work – for now focus on making it work functionally. Mark this section with # TODO: Re-register agent as MLflow model if needed for deployment.
Removing DSPy and Legacy Code
•	Uninstall/Remove DSPy: Ensure that dspy is no longer imported anywhere @04_DSPy Orchestration.py. Remove import lines like import dspy and any uses of dspy.Signature or dspy.Module. The new code should rely only on standard libraries or Databricks SDK/MLflow.
•	Prune unused modules: If streaming_document_pipeline.py, old vector_search_manager.py, or other legacy files are no longer needed, consider deleting them (or at least stop referencing them in the project). The new pipeline (PDF ETL script + new agent logic) supersedes them. You might keep them temporarily for reference, but mark with comments like # TODO: Delete this file once new pipeline is confirmed working.
•	Update references: Search the codebase for any references to chunked_text, chunk_id, or other text-era artifacts. Update them to new image-based terms or remove entirely. For example, if chunked_text appears in retrieval or prompt templates, eliminate it vector_search_manager.py. We will not format image context as text passages, so any prompt template like “Passage: {chunk_text}” in config can be removed/ swapped for the images encoded files themselves or left unused @global_config.py.
•	Document code: Add comments where logic has changed for clarity. E.g., in the agent code, note that we are now using base64 images as context for the LLM instead of text: # Context is provided via images (base64), not text chunks. This helps future maintainers (and your Git history will show the diff).
Validation & Testing
•	Deploy and run end-to-end: Launch the refactored pipeline in a controlled environment. For Databricks, this might mean running the notebooks or scripts in order:
1.	We will deploy CLIP model (if not already) – run the updated driver.py or 01_CLIP_Deployment.py to ensure the serving endpoint is active (I will take care of this).
2.	Run the PDF processing module to populate the clip_pdf_pages table (verify the Delta table has rows with base64 images).
3.	Run the embedding generation to fill the embeddings table. Confirm the table has vector data.
4.	Create the vector index (if not auto-created).
•	Test retrieval query: Manually run a test query through the new agent logic. For example:
python
query = "¿Qué es un CDV?"  
answer = multimodal_agent.predict(query)  
print(answer)
This should internally get the query embedding, retrieve images, call GPT-4o, and return an answer. Check that the answer is relevant and that the process runs without errors. If there are issues, use logging or print statements in each step (embedding, retrieval, LLM call) to debug.
•	Image handling check: We will verify that the LLM is indeed utilizing the images. For instance, if user queries something that requires reading the image (e.g., “What does the diagram show on page 2 of Document X?”), the answer should reflect the image content. If you consistently get “I cannot read images” or similar, then the integration with GPT-4o might be incorrect – we will adjust how the image is provided to the model.
•	Performance test:I will try the pipeline and revert back with any error we need to fix.
•	Integration in Cursor: Since developers will use Cursor with Git, confirm that all required environment variables or config files are present so that further developing the RAG system is well integrated with cursor. For example, if using environment variables for secrets (like Azure OpenAI keys or Databricks host tokens), document these in a .env.example or README for the Cursor setup.
•	Finalize cleanup: Remove any leftover TODO comments that have been addressed or convert them to issues if needed. Merge the feature branch back to main once validated.
Additional Notes
•	GPT-4o model prompts: You might want to craft a clear system prompt for GPT-4o when analyzing an image (e.g., “You are an AI assistant with vision. You are given an image and a question. Answer based on the image content. You are an insightful and helpful assistant for BCP\that only answers questions related to BCP internal documentation. Use the following\pieces of retrieved context to answer the question. Some pieces of context may\be irrelevant, in which case you should not use them to form the answer. Answer\honestly and if you do not now the answer or if the answer is not contained\in the documentation provided as context, limit yourself to answer that \"You\could not find the answer in the documentation and prompt the user to provide\\ more details\"\n\n”). This can be stored in config (similar to the old llm_system_prompt_template global_config.py) and prepended to the chat messages.
•	Future enhancements: Once this refactor is stable, consider packaging the entire pipeline (from ingestion to answer) as a single MLflow model or a Databricks Asset Bundle, so it can be easily triggered. But first, ensure DSPy is fully removed and the new architecture works reliably with base64 images end-to-end.
